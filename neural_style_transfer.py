# -*- coding: utf-8 -*-
"""Neural Style Transfer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrM9d1Kk2M1PyDfjmo_GfpdkO1qjv0s9
"""

# 上傳圖片區
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print(f'User uploaded file "{fn}" with length {len(uploaded[fn])} bytes')

# ==================================
# 1. 匯入會用到的函式庫
# ==================================
import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

print(f"TensorFlow Version: {tf.__version__}")

# ==================================
# 2. 圖片前處理函式
# ==================================
def load_and_preprocess_image(path_to_img, max_dim=256):
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    shape = tf.cast(tf.shape(img)[:-1], tf.float32)
    long_dim = max(shape)
    scale = max_dim / long_dim
    new_shape = tf.cast(shape * scale, tf.int32)
    img = tf.image.resize(img, new_shape)
    img = img[tf.newaxis, :]
    return img

def tensor_to_image(tensor):
    tensor = tensor * 255
    tensor = np.array(tensor, dtype=np.uint8)
    if np.ndim(tensor) > 3:
        assert tensor.shape[0] == 1
        tensor = tensor[0]
    return Image.fromarray(tensor)

# ==================================
# 3. 載入內容和風格圖片
# ==================================
content_path = 'content.jpg'
style_path = 'style.jpg'

content_image = load_and_preprocess_image(content_path)
style_image = load_and_preprocess_image(style_path)

print("內容圖片與風格圖片已載入並預處理完成。")
plt.subplot(1, 2, 1)
plt.imshow(content_image[0])
plt.title('Content Image')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(style_image[0])
plt.title('Style Image')
plt.axis('off')
plt.show()

# ==================================
# 4. 建立模型與定義損失函數 (核心部分)
# ==================================
# 載入 VGG19 模型
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False

# 定義特徵層
content_layers = ['block5_conv2']
style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# 建立特徵提取器模型
def vgg_layers(layer_names):
    outputs = [vgg.get_layer(name).output for name in layer_names]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# Gram 矩陣計算函式
def gram_matrix(input_tensor):
    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result/(num_locations)

# 建立特徵提取器實例
style_extractor = vgg_layers(style_layers)
content_extractor = vgg_layers(content_layers)

# 定義風格和內容的權重
style_weight=1e-2
content_weight=1e4

# 修正後的損失函數
def style_content_loss(style_outputs_grams, content_outputs, style_targets, content_targets):
    # 手動展開風格損失的計算 (style_layers 有 5 層)
    style_loss = (tf.reduce_mean((style_outputs_grams[0] - style_targets[0])**2) +
                  tf.reduce_mean((style_outputs_grams[1] - style_targets[1])**2) +
                  tf.reduce_mean((style_outputs_grams[2] - style_targets[2])**2) +
                  tf.reduce_mean((style_outputs_grams[3] - style_targets[3])**2) +
                  tf.reduce_mean((style_outputs_grams[4] - style_targets[4])**2))

    style_loss *= style_weight / num_style_layers

    # 手動展開內容損失的計算 (content_layers 有 1 層)
    # 由於 content_outputs 和 content_targets 都只有一個元素，直接取 [0]
    content_loss = tf.reduce_mean((content_outputs[0] - content_targets[0])**2)

    content_loss *= content_weight / num_content_layers

    loss = style_loss + content_loss
    return loss

# ==================================
# 5. 準備訓練並定義訓練步驟 (恢復成原始版本)
# ==================================
# 預先計算目標特徵
style_outputs = style_extractor(style_image*255)
content_outputs = content_extractor(content_image*255)
style_targets = [gram_matrix(style_output) for style_output in style_outputs]
content_targets = [content_output for content_output in content_outputs]

# 定義要優化的圖片，並初始化
image = tf.Variable(content_image)

# 定義優化器
optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

# 恢復成最簡單的 @tf.function 版本
@tf.function()
def train_step(image):
    with tf.GradientTape() as tape:
        style_out = style_extractor(image*255)
        content_out = content_extractor(image*255)

        style_out_grams = [gram_matrix(s) for s in style_out]

        loss = style_content_loss(style_out_grams, content_out, style_targets, content_targets)

    grad = tape.gradient(loss, image)
    optimizer.apply_gradients([(grad, image)])
    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))

print("前置作業完成")

# ==================================
# 6. 開始進行風格轉換！
# ==================================
epochs = 5
steps_per_epoch = 50

print("\n開始進行風格轉換 請稍候...")

# 執行主迴圈
for n in range(epochs):
    for m in range(steps_per_epoch):
      train_step(image)

    # 每一個 epoch 結束後，顯示當前進度
    display_image = tensor_to_image(image)
    print(f"Epoch {n+1}/{epochs} 完成")
    plt.imshow(display_image)
    plt.title(f'Epoch {n+1}/{epochs}')
    plt.axis('off')
    plt.show()

print("風格轉換完成！")

# 顯示並儲存最終成果
final_image = tensor_to_image(image)
final_image.save('final_stylized_image.png')

print("\n最終成果：")
plt.figure(figsize=(10,10))
plt.imshow(final_image)
plt.title('Final Stylized Image')
plt.axis('off')
plt.show()
