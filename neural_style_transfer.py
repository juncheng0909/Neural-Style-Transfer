# -*- coding: utf-8 -*-
"""Neural Style Transfer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrM9d1Kk2M1PyDfjmo_GfpdkO1qjv0s9
"""

# ä¸Šå‚³åœ–ç‰‡å€
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print(f'User uploaded file "{fn}" with length {len(uploaded[fn])} bytes')

# ==================================
# 1. åŒ¯å…¥æœƒç”¨åˆ°çš„å‡½å¼åº«
# ==================================
import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

print(f"TensorFlow Version: {tf.__version__}")

# ==================================
# 2. åœ–ç‰‡å‰è™•ç†å‡½å¼
# ==================================
def load_and_preprocess_image(path_to_img, max_dim=256):
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    shape = tf.cast(tf.shape(img)[:-1], tf.float32)
    long_dim = max(shape)
    scale = max_dim / long_dim
    new_shape = tf.cast(shape * scale, tf.int32)
    img = tf.image.resize(img, new_shape)
    img = img[tf.newaxis, :]
    return img

def tensor_to_image(tensor):
    tensor = tensor * 255
    tensor = np.array(tensor, dtype=np.uint8)
    if np.ndim(tensor) > 3:
        assert tensor.shape[0] == 1
        tensor = tensor[0]
    return Image.fromarray(tensor)

# ==================================
# 3. è¼‰å…¥å…§å®¹å’Œé¢¨æ ¼åœ–ç‰‡
# ==================================
content_path = 'content.jpg'
style_path = 'style.jpg'

content_image = load_and_preprocess_image(content_path)
style_image = load_and_preprocess_image(style_path)

print("å…§å®¹åœ–ç‰‡èˆ‡é¢¨æ ¼åœ–ç‰‡å·²è¼‰å…¥ä¸¦é è™•ç†å®Œæˆã€‚")
plt.subplot(1, 2, 1)
plt.imshow(content_image[0])
plt.title('Content Image')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(style_image[0])
plt.title('Style Image')
plt.axis('off')
plt.show()

# ==================================
# 4. å»ºç«‹æ¨¡å‹èˆ‡å®šç¾©æå¤±å‡½æ•¸ (æ ¸å¿ƒéƒ¨åˆ†)
# ==================================
# è¼‰å…¥ VGG19 æ¨¡å‹
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False

# å®šç¾©ç‰¹å¾µå±¤
content_layers = ['block5_conv2']
style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

# å»ºç«‹ç‰¹å¾µæå–å™¨æ¨¡å‹
def vgg_layers(layer_names):
    outputs = [vgg.get_layer(name).output for name in layer_names]
    model = tf.keras.Model([vgg.input], outputs)
    return model

# Gram çŸ©é™£è¨ˆç®—å‡½å¼
def gram_matrix(input_tensor):
    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
    input_shape = tf.shape(input_tensor)
    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
    return result/(num_locations)

# å»ºç«‹ç‰¹å¾µæå–å™¨å¯¦ä¾‹
style_extractor = vgg_layers(style_layers)
content_extractor = vgg_layers(content_layers)

# å®šç¾©é¢¨æ ¼å’Œå…§å®¹çš„æ¬Šé‡
style_weight=1e-2
content_weight=1e4

# ã€ä¿®æ­£å¾Œçš„æå¤±å‡½æ•¸ã€‘
def style_content_loss(style_outputs_grams, content_outputs, style_targets, content_targets):
    # æ‰‹å‹•å±•é–‹é¢¨æ ¼æå¤±çš„è¨ˆç®— (style_layers æœ‰ 5 å±¤)
    style_loss = (tf.reduce_mean((style_outputs_grams[0] - style_targets[0])**2) +
                  tf.reduce_mean((style_outputs_grams[1] - style_targets[1])**2) +
                  tf.reduce_mean((style_outputs_grams[2] - style_targets[2])**2) +
                  tf.reduce_mean((style_outputs_grams[3] - style_targets[3])**2) +
                  tf.reduce_mean((style_outputs_grams[4] - style_targets[4])**2))

    style_loss *= style_weight / num_style_layers

    # æ‰‹å‹•å±•é–‹å…§å®¹æå¤±çš„è¨ˆç®— (content_layers æœ‰ 1 å±¤)
    # ç”±æ–¼ content_outputs å’Œ content_targets éƒ½åªæœ‰ä¸€å€‹å…ƒç´ ï¼Œç›´æ¥å– [0]
    content_loss = tf.reduce_mean((content_outputs[0] - content_targets[0])**2)

    content_loss *= content_weight / num_content_layers

    loss = style_loss + content_loss
    return loss

# ==================================
# 5. æº–å‚™è¨“ç·´ä¸¦å®šç¾©è¨“ç·´æ­¥é©Ÿ (æ¢å¾©æˆåŸå§‹ç‰ˆæœ¬)
# ==================================
# é å…ˆè¨ˆç®—ç›®æ¨™ç‰¹å¾µ
style_outputs = style_extractor(style_image*255)
content_outputs = content_extractor(content_image*255)
style_targets = [gram_matrix(style_output) for style_output in style_outputs]
content_targets = [content_output for content_output in content_outputs]

# å®šç¾©è¦å„ªåŒ–çš„åœ–ç‰‡ï¼Œä¸¦åˆå§‹åŒ–
image = tf.Variable(content_image)

# å®šç¾©å„ªåŒ–å™¨
optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

# ã€æ¢å¾©æˆæœ€ç°¡å–®çš„ @tf.function ç‰ˆæœ¬ã€‘
@tf.function()
def train_step(image):
    with tf.GradientTape() as tape:
        style_out = style_extractor(image*255)
        content_out = content_extractor(image*255)

        style_out_grams = [gram_matrix(s) for s in style_out]

        loss = style_content_loss(style_out_grams, content_out, style_targets, content_targets)

    grad = tape.gradient(loss, image)
    optimizer.apply_gradients([(grad, image)])
    image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))

print("âœ… è¨“ç·´å‰ç½®ä½œæ¥­æº–å‚™å®Œæˆï¼")

# ==================================
# 6. é–‹å§‹é€²è¡Œé¢¨æ ¼è½‰æ›ï¼
# ==================================
epochs = 5
steps_per_epoch = 50

print("\nğŸš€ é–‹å§‹é€²è¡Œé¢¨æ ¼è½‰æ›ï¼Œé€™å€‹éç¨‹æœƒéœ€è¦å¹¾åˆ†é˜ï¼Œè«‹ç¨å€™...")

# åŸ·è¡Œä¸»è¿´åœˆ
for n in range(epochs):
    for m in range(steps_per_epoch):
      train_step(image)

    # æ¯ä¸€å€‹ epoch çµæŸå¾Œï¼Œé¡¯ç¤ºç•¶å‰é€²åº¦
    display_image = tensor_to_image(image)
    print(f"Epoch {n+1}/{epochs} å®Œæˆ")
    plt.imshow(display_image)
    plt.title(f'Epoch {n+1}/{epochs}')
    plt.axis('off')
    plt.show()

print("âœ… é¢¨æ ¼è½‰æ›å®Œæˆï¼")

# é¡¯ç¤ºä¸¦å„²å­˜æœ€çµ‚æˆæœ
final_image = tensor_to_image(image)
final_image.save('final_stylized_image.png')

print("\nğŸ‰ æœ€çµ‚æˆæœï¼š")
plt.figure(figsize=(10,10))
plt.imshow(final_image)
plt.title('Final Stylized Image')
plt.axis('off')
plt.show()